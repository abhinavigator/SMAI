{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imMHq8PVcEFb"
   },
   "source": [
    "## Image Captioning using KNN\n",
    "\n",
    "Although VLMs (Vision Language Models) are the go to tools for image captioning right now, there are interesting works from earlier years that used KNN for captioning and perform surprisingly well enough!\n",
    "\n",
    "Further, Libraries like [Faiss](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) perform the nearest neighbor computation efficiently and are used in many industrial applications.\n",
    "\n",
    "- In this question you will implement an algorithm to perform captioning using KNN based on the paper [A Distributed Representation Based Query Expansion Approach for\n",
    "Image Captioning](https://aclanthology.org/P15-2018.pdf)\n",
    "\n",
    "- Dataset: [MS COCO](https://cocodataset.org/#home) 2014 (val set only)\n",
    "\n",
    "- Algorithm:\n",
    "    1. Given: Image embeddings and correspond caption embeddings (5 Per image)\n",
    "    1. For every image, findout the k nearest images and compute its query vector as the weighted sum of the captions of the nearest images (k*5 captions per image)\n",
    "    1. The predicted caption would be the caption in the dataset that is closest to the query vector. (for the sake of the assignment use the same coco val set captions as the dataset)\n",
    "\n",
    "- The image and text embeddings are extracted from the [CLIP](https://openai.com/research/clip) model. (You need not know about this right now)\n",
    "\n",
    "- Tasks:\n",
    "    1. Implement the algorithm and compute the bleu score. Use Faiss for nearest neighbor computation. Starter code is provided below.\n",
    "    1. Try a few options for k. Record your observations.\n",
    "    1. For a fixed k, try a few options in the Faiss index factory to speed the computation in step 2. Record your observations.\n",
    "    1. Qualitative study: Visualize five images, their ground truth captions and the predicted caption.\n",
    "    \n",
    "Note: Run this notebook on Colab for fastest resu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0XAWcEuKvyG",
    "outputId": "8d5eee2c-db1d-4088-f0c5-ec4c5b73d690"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: gdown\n"
     ]
    }
   ],
   "source": [
    "!gdown 1RwhwntZGZ9AX8XtGIDAcQD3ByTcUiOoO #image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sWjuVTJkDvM4",
    "outputId": "42b13b0b-b0e2-4443-f4ae-37ea61f768b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: gdown\n"
     ]
    }
   ],
   "source": [
    "!gdown 1b-4hU2Kp93r1nxMUGEgs1UbZov0OqFfW #caption embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P7h8e8DunDEK",
    "outputId": "511fb666-1edb-4629-84ca-12469610993b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-01 18:54:39--  http://images.cocodataset.org/zips/val2014.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.214.33, 52.217.121.49, 16.182.66.9, ...\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.214.33|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 6645013297 (6.2G) [application/zip]\n",
      "Saving to: ‘val2014.zip’\n",
      "\n",
      "val2014.zip         100%[===================>]   6.19G  3.47MB/s    in 53m 20s \n",
      "\n",
      "2024-02-01 19:48:00 (1.98 MB/s) - ‘val2014.zip’ saved [6645013297/6645013297]\n",
      "\n",
      "unzip:  cannot find or open /content/val2014.zip, /content/val2014.zip.zip or /content/val2014.zip.ZIP.\n",
      "--2024-02-01 19:48:00--  http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
      "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.221.81, 52.216.169.123, 16.182.98.161, ...\n",
      "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.221.81|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 252872794 (241M) [application/zip]\n",
      "Saving to: ‘annotations_trainval2014.zip’\n",
      "\n",
      "annotations_trainva 100%[===================>] 241.16M  3.40MB/s    in 5m 57s  \n",
      "\n",
      "2024-02-01 19:53:58 (691 KB/s) - ‘annotations_trainval2014.zip’ saved [252872794/252872794]\n",
      "\n",
      "unzip:  cannot find or open /content/annotations_trainval2014.zip, /content/annotations_trainval2014.zip.zip or /content/annotations_trainval2014.zip.ZIP.\n",
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/site-packages (1.7.4)\n"
     ]
    }
   ],
   "source": [
    "!wget http://images.cocodataset.org/zips/val2014.zip\n",
    "!unzip /content/val2014.zip\n",
    "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
    "!unzip /content/annotations_trainval2014.zip\n",
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open /content/annotations_trainval2014.zip, /content/annotations_trainval2014.zip.zip or /content/annotations_trainval2014.zip.ZIP.\n"
     ]
    }
   ],
   "source": [
    "!unzip /content/annotations_trainval2014.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.11/site-packages (1.7.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "jI9PuLZ6b51J"
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.translate import bleu_score\n",
    "import faiss\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "igqtoD36iQv8",
    "outputId": "8c0b167b-b772-438f-dda1-e2c556d176f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.35s)\n",
      "creating index...\n",
      "index created!\n",
      "Number of samples:  40504\n",
      "Image Size:  torch.Size([3, 224, 224])\n",
      "['A loft bed with a dresser underneath it.', 'A bed and desk in a small room.', 'Wooden bed on top of a white dresser.', 'A bed sits on top of a dresser and a desk.', 'Bunk bed with a narrow shelf sitting underneath it. ']\n"
     ]
    }
   ],
   "source": [
    "def get_transform():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),  # convert the PIL Image to a tensor\n",
    "        transforms.Normalize(\n",
    "            (0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "            (0.229, 0.224, 0.225),\n",
    "        )\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "coco_dset = dset.CocoCaptions(root = '/Users/abhinavanand/Downloads/rollnumber_A1-2/rollnumber_A1_Q2/val2014',\n",
    "                        annFile = '/Users/abhinavanand/Downloads/rollnumber_A1-2/rollnumber_A1_Q2/annotations/captions_val2014.json',\n",
    "                        transform=get_transform())\n",
    "\n",
    "print('Number of samples: ', len(coco_dset))\n",
    "img, target = coco_dset[3] # load 4th sample\n",
    "\n",
    "print(\"Image Size: \", img.shape)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UpjbQ_KUick0",
    "outputId": "9c3f3d6e-757e-4e23-c613-0bd02320fc71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions: (40504, 5)\n"
     ]
    }
   ],
   "source": [
    "ids = list(sorted(coco_dset.coco.imgs.keys()))\n",
    "captions = []\n",
    "for i in range(len(ids)):\n",
    "    captions.append([ele['caption'] for ele in coco_dset.coco.loadAnns(coco_dset.coco.getAnnIds(ids[i]))][:5]) #5 per image\n",
    "captions_np = np.array(captions)\n",
    "print('Captions:', captions_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "alKCWuRhivc_",
    "outputId": "240b9882-ac13-4f15-c074-ed9da4446f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total captions: 202520\n"
     ]
    }
   ],
   "source": [
    "captions_flat = captions_np.flatten().tolist()\n",
    "print('Total captions:', len(captions_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "leEngJ4hkzpD",
    "outputId": "d47a6141-d268-49c8-e84d-520ca0c678b3"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/content/coco_captions.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/abhinavanand/Downloads/rollnumber_A1-2/rollnumber_A1_Q2/Q2.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/abhinavanand/Downloads/rollnumber_A1-2/rollnumber_A1_Q2/Q2.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cap_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/content/coco_captions.npy\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/abhinavanand/Downloads/rollnumber_A1-2/rollnumber_A1_Q2/Q2.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m caption_embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(cap_path)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/abhinavanand/Downloads/rollnumber_A1-2/rollnumber_A1_Q2/Q2.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mCaption embeddings\u001b[39m\u001b[39m'\u001b[39m,caption_embeddings\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/coco_captions.npy'"
     ]
    }
   ],
   "source": [
    "cap_path = '/content/coco_captions.npy'\n",
    "caption_embeddings = np.load(cap_path)\n",
    "print('Caption embeddings',caption_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JnAdVULXlTNV",
    "outputId": "5a9f9a0d-678d-4a14-ae2b-e6c9d0ec4a13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image embeddings (40504, 512)\n"
     ]
    }
   ],
   "source": [
    "img_path = '/content/coco_imgs.npy'\n",
    "image_embeddings = np.load(img_path)\n",
    "print('Image embeddings',image_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VXzKkgt-lhTo"
   },
   "outputs": [],
   "source": [
    "def accuracy(predict, real):\n",
    "    '''\n",
    "    use bleu score as a measurement of accuracy\n",
    "    :param predict: a list of predicted captions\n",
    "    :param real: a list of actual descriptions\n",
    "    :return: bleu accuracy\n",
    "    '''\n",
    "    accuracy = 0\n",
    "    for i, pre in enumerate(predict):\n",
    "        references = real[i]\n",
    "        score = bleu_score.sentence_bleu(references, pre)\n",
    "        accuracy += score\n",
    "    return accuracy/len(predict)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
